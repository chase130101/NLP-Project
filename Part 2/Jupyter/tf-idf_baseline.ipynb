{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parameters_lstm import *\n",
    "\n",
    "from preprocess_datapoints_lstm import *\n",
    "from preprocess_text_to_tensors_lstm import *\n",
    "from meter import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CountVectorizer\n",
    "\n",
    "''' Data processing helpers only for TF-IDF implementation '''\n",
    "\n",
    "# Processes all sentences in out datasets to give useful containers of data concerning the corpus:\n",
    "# word2id vocab\n",
    "# dict of question id to list of words in the question\n",
    "# Processes all sentences in out datasets to give useful containers of data concerning the corpus:\n",
    "# word2id vocab\n",
    "# dict of question id to list of words in the question\n",
    "def process_only_android_corpus():\n",
    "    dataset_path = android_corpus_path\n",
    "    all_txt = []\n",
    "    all_txt_title = []\n",
    "    all_txt_body = []\n",
    "    android_id_to_data_title = {}\n",
    "    android_id_to_data_body = {}\n",
    "\n",
    "    lines = open(dataset_path, encoding = 'utf8').readlines()\n",
    "    for line in lines:\n",
    "\n",
    "        id_title_body_list = line.split('\\t')\n",
    "        idx = int(id_title_body_list[0])\n",
    "        title_plus_body = id_title_body_list[1] + ' ' + id_title_body_list[2][:-1]\n",
    "        all_txt.append(title_plus_body)\n",
    "        all_txt_title.append(id_title_body_list[1])\n",
    "        all_txt_body.append(id_title_body_list[2])\n",
    "\n",
    "        android_id_to_data_title[idx] = id_title_body_list[1]\n",
    "        android_id_to_data_body[idx] = id_title_body_list[2]\n",
    "\n",
    "    # vectorizer = CountVectorizer(binary=True, analyzer='word', token_pattern='[^\\s]+[a-z]*[0-9]*')\n",
    "    vectorizer = CountVectorizer(binary=True, analyzer='word', max_df=0.2)\n",
    "\n",
    "    vectorizer.fit(all_txt)\n",
    "\n",
    "    return {\n",
    "            'word_to_id': vectorizer.vocabulary_,\n",
    "            'android_id_to_data_title': android_id_to_data_title,\n",
    "            'android_id_to_data_body': android_id_to_data_body,\n",
    "            'vectorizer': vectorizer\n",
    "            }\n",
    "\n",
    "\n",
    "''' Data Sets '''\n",
    "processed_corpus = process_only_android_corpus()\n",
    "android_id_to_data_title = processed_corpus['android_id_to_data_title']\n",
    "android_id_to_data_body = processed_corpus['android_id_to_data_body']\n",
    "vectorizer = processed_corpus['vectorizer']\n",
    "\n",
    "test_data_android = android_id_to_similar_different(dev=False)\n",
    "test_question_ids_android = list(test_data_android.keys())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000 38000 39000 40000 41000 42000 43000 44000 45000 46000 47000 48000 49000 50000 51000 52000 53000 54000 55000 56000 57000 58000 59000 60000 61000 62000 63000 64000 65000 66000 67000 68000 69000 70000 71000 72000 73000 74000 75000 76000 77000 78000 79000 80000 81000 82000 83000 84000 85000 86000 87000 88000 89000 90000 91000 92000 93000 94000 95000 96000 97000 98000 99000 100000 101000 102000 103000 104000 105000 106000 107000 108000 109000 110000 111000 112000 113000 114000 115000 116000 117000 118000 119000 AUC Score using TF-IDF: 0.629945006105\n"
     ]
    }
   ],
   "source": [
    "auc_scorer = AUCMeter()\n",
    "\n",
    "''' Begin Evaluation'''\n",
    "candidate_ids, q_main_ids, labels = organize_test_ids(test_question_ids_android, test_data_android)\n",
    "list_of_scores = []\n",
    "\n",
    "index_into_list_all_all_candidate_ids = 0\n",
    "q_main_id_ind = 0\n",
    "for q_main_id in q_main_ids:\n",
    "    if index_into_list_all_all_candidate_ids % 1000 == 0:  \n",
    "        print(index_into_list_all_all_candidate_ids, end = ' ')\n",
    "    q_main_sentence_title = android_id_to_data_title[q_main_id]\n",
    "    q_main_sentence_body = android_id_to_data_body[q_main_id]\n",
    "    q_main_vector_title = torch.from_numpy((vectorizer.transform([q_main_sentence_title]).toarray()[0])).float().unsqueeze(0)\n",
    "    q_main_vector_body = torch.from_numpy((vectorizer.transform([q_main_sentence_body]).toarray()[0])).float().unsqueeze(0)\n",
    "    q_main_vector = (q_main_vector_title + q_main_vector_body)/2\n",
    "\n",
    "    q_candidate_id = candidate_ids[index_into_list_all_all_candidate_ids]\n",
    "    q_candidate_sentence_title = android_id_to_data_title[q_candidate_id]\n",
    "    q_candidate_sentence_body = android_id_to_data_body[q_candidate_id]\n",
    "    q_candidate_vector_title = torch.from_numpy((vectorizer.transform([q_candidate_sentence_title]).toarray()[0])).float().unsqueeze(0)\n",
    "    q_candiate_vector_body = torch.from_numpy((vectorizer.transform([q_candidate_sentence_body]).toarray()[0])).float().unsqueeze(0)\n",
    "    q_candidate_vector = (q_candidate_vector_title + q_candiate_vector_body)/2\n",
    "\n",
    "    score_cos_sim = Variable(torch.nn.functional.cosine_similarity(q_candidate_vector, q_main_vector)).data[0]\n",
    "    list_of_scores.append(score_cos_sim)\n",
    "\n",
    "    index_into_list_all_all_candidate_ids += 1\n",
    "        \n",
    "\n",
    "target = torch.FloatTensor(labels)\n",
    "auc_scorer.reset()\n",
    "auc_scorer.add(torch.FloatTensor(list_of_scores), target)\n",
    "auc_score = auc_scorer.value()\n",
    "\n",
    "print(\"AUC Score using TF-IDF:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parameters_lstm2 import *\n",
    "\n",
    "from preprocess_datapoints_lstm2 import *\n",
    "from preprocess_text_to_tensors_lstm2 import *\n",
    "from meter import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TfidfVectorizer\n",
    "\n",
    "''' Data processing helpers only for TF-IDF implementation '''\n",
    "\n",
    "# Processes all sentences in out datasets to give useful containers of data concerning the corpus:\n",
    "# word2id vocab\n",
    "# dict of question id to list of words in the question\n",
    "# Processes all sentences in out datasets to give useful containers of data concerning the corpus:\n",
    "# word2id vocab\n",
    "# dict of question id to list of words in the question\n",
    "def process_only_android_corpus():\n",
    "    dataset_path = android_corpus_path\n",
    "    all_txt = []\n",
    "    android_id_to_data = {}\n",
    "\n",
    "    lines = open(dataset_path, encoding = 'utf8').readlines()\n",
    "    for line in lines:\n",
    "\n",
    "        id_title_body_list = line.split('\\t')\n",
    "        idx = int(id_title_body_list[0])\n",
    "        title_plus_body = id_title_body_list[1] + ' ' + id_title_body_list[2][:-1]\n",
    "        all_txt.append(title_plus_body)\n",
    "\n",
    "        android_id_to_data[idx] = title_plus_body[1]\n",
    "\n",
    "    # vectorizer = CountVectorizer(binary=True, analyzer='word', token_pattern='[^\\s]+[a-z]*[0-9]*')\n",
    "    vectorizer = TfidfVectorizer(binary=True, analyzer='word', max_df=0.2)\n",
    "\n",
    "    vectorizer.fit(all_txt)\n",
    "\n",
    "    return {\n",
    "            'word_to_id': vectorizer.vocabulary_,\n",
    "            'android_id_to_data': android_id_to_data_title,\n",
    "            'vectorizer': vectorizer\n",
    "            }\n",
    "\n",
    "\n",
    "''' Data Sets '''\n",
    "processed_corpus = process_only_android_corpus()\n",
    "android_id_to_data = processed_corpus['android_id_to_data']\n",
    "vectorizer = processed_corpus['vectorizer']\n",
    "\n",
    "test_data_android = android_id_to_similar_different(dev=False)\n",
    "test_question_ids_android = list(test_data_android.keys())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000 38000 39000 40000 41000 42000 43000 44000 45000 46000 47000 48000 49000 50000 51000 52000 53000 54000 55000 56000 57000 58000 59000 60000 61000 62000 63000 64000 65000 66000 67000 68000 69000 70000 71000 72000 73000 74000 75000 76000 77000 78000 79000 80000 81000 82000 83000 84000 85000 86000 87000 88000 89000 90000 91000 92000 93000 94000 95000 96000 97000 98000 99000 100000 101000 102000 103000 104000 105000 106000 107000 108000 109000 110000 111000 112000 113000 114000 115000 116000 117000 118000 119000 AUC Score using TF-IDF: 0.543156311558\n"
     ]
    }
   ],
   "source": [
    "auc_scorer = AUCMeter()\n",
    "\n",
    "''' Begin Evaluation'''\n",
    "candidate_ids, q_main_ids, labels = organize_test_ids(test_question_ids_android, test_data_android)\n",
    "list_of_scores = []\n",
    "\n",
    "index_into_list_all_all_candidate_ids = 0\n",
    "q_main_id_ind = 0\n",
    "for q_main_id in q_main_ids:\n",
    "    if index_into_list_all_all_candidate_ids % 1000 == 0:  \n",
    "        print(index_into_list_all_all_candidate_ids, end = ' ')\n",
    "    q_main_sentence = android_id_to_data_body[q_main_id]\n",
    "    q_main_vector = torch.from_numpy((vectorizer.transform([q_main_sentence]).toarray()[0])).float().unsqueeze(0)\n",
    "\n",
    "    q_candidate_id = candidate_ids[index_into_list_all_all_candidate_ids]\n",
    "    q_candidate_sentence = android_id_to_data[q_candidate_id]\n",
    "    q_candidate_vector = torch.from_numpy((vectorizer.transform([q_candidate_sentence]).toarray()[0])).float().unsqueeze(0)\n",
    "\n",
    "    score_cos_sim = Variable(torch.nn.functional.cosine_similarity(q_candidate_vector, q_main_vector)).data[0]\n",
    "    list_of_scores.append(score_cos_sim)\n",
    "\n",
    "    index_into_list_all_all_candidate_ids += 1\n",
    "        \n",
    "\n",
    "target = torch.FloatTensor(labels)\n",
    "auc_scorer.reset()\n",
    "auc_scorer.add(torch.FloatTensor(list_of_scores), target)\n",
    "auc_score = auc_scorer.value()\n",
    "\n",
    "print(\"AUC Score using TF-IDF:\", auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
