{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parameters_lstm import *\n",
    "\n",
    "from preprocess_datapoints_lstm import *\n",
    "from preprocess_text_to_tensors_lstm import *\n",
    "from meter import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "saved_model_name = 'lstm_dir_trans'\n",
    "\n",
    "# Initialize the data sets\n",
    "processed_corpus = process_whole_corpuses()\n",
    "word_to_id_vocab = processed_corpus['word_to_id']\n",
    "word2vec = load_glove_embeddings(glove_path, word_to_id_vocab)\n",
    "ubuntu_id_to_data_title = processed_corpus['ubuntu_id_to_data_title']\n",
    "android_id_to_data_title = processed_corpus['android_id_to_data_title']\n",
    "ubuntu_id_to_data_body = processed_corpus['ubuntu_id_to_data_body']\n",
    "android_id_to_data_body = processed_corpus['android_id_to_data_body']\n",
    "\n",
    "\n",
    "''' Data Sets '''\n",
    "training_data_ubuntu = ubuntu_id_to_similar_different()\n",
    "training_question_ids_ubuntu = list(training_data_ubuntu.keys())\n",
    "dev_data_android = android_id_to_similar_different(dev=True)\n",
    "dev_question_ids_android = list(dev_data_android.keys())\n",
    "test_data_android = android_id_to_similar_different(dev=False)\n",
    "test_question_ids_android = list(test_data_android.keys())\n",
    "# Note: Remember to edit batch_size accordingly if testing on smaller size data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(lstm, ids, data, word2vec, id2Data_title, id2Data_body, word_to_id_vocab, truncation_val_title, truncation_val_body):\n",
    "    lstm.eval()\n",
    "    auc_scorer.reset()\n",
    "\n",
    "    candidate_ids, q_main_ids, labels = organize_test_ids(ids, data)\n",
    "    num_q_main = len(q_main_ids)\n",
    "    len_pieces = round(num_q_main/50)\n",
    "    print(num_q_main)\n",
    "\n",
    "    for i in range(0, num_q_main, len_pieces):\n",
    "        print(i, end = ' ')\n",
    "        q_main_id_num_repl_tuple = q_main_ids[i:i+len_pieces]\n",
    "        candidates = candidate_ids[i:i+len_pieces]\n",
    "        current_labels = torch.from_numpy(np.array(labels[i:i+len_pieces])).long()\n",
    "\n",
    "        candidates_qs_matrix = construct_qs_matrix_testing(candidates, lstm, h0, c0, word2vec, id2Data_title, id2Data_body,\n",
    "        word_to_id_vocab, truncation_val_title, truncation_val_body, main=False)\n",
    "        main_qs_matrix = construct_qs_matrix_testing(q_main_id_num_repl_tuple, lstm, h0, c0, word2vec, id2Data_title, id2Data_body,\n",
    "        word_to_id_vocab, truncation_val_title, truncation_val_body, main=True)\n",
    "\n",
    "        similarity_matrix_this_batch = torch.nn.functional.cosine_similarity(candidates_qs_matrix, main_qs_matrix, eps=1e-08).data\n",
    "        auc_scorer.add(similarity_matrix_this_batch, current_labels)\n",
    "\n",
    "    auc_score = auc_scorer.value()\n",
    "\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Params Dashboard '''\n",
    "\n",
    "''' Procedural parameters '''\n",
    "batch_size = 40\n",
    "num_differing_questions = 20\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "''' Model specs LSTM '''\n",
    "dropout = 0.2\n",
    "margin = 0.15\n",
    "lr_lstm = 10**-3\n",
    "\n",
    "input_size = 300\n",
    "hidden_size = 240\n",
    "num_layers = 1\n",
    "bias = True\n",
    "batch_first = True\n",
    "bidirectional = True\n",
    "first_dim = num_layers * 2 if bidirectional else num_layers\n",
    "\n",
    "\n",
    "''' Model specs NN '''\n",
    "lr_nn = -10**-4\n",
    "lamb = 10**-3\n",
    "\n",
    "input_size_nn = 2*hidden_size if bidirectional else hidden_size\n",
    "first_hidden_size_nn = 300\n",
    "second_hidden_size_nn = 150\n",
    "\n",
    "\n",
    "''' Data processing specs '''\n",
    "truncation_val_title = 15\n",
    "truncation_val_body = 85\n",
    "padding_idx = 0\n",
    "\n",
    "glove_path = '../glove.840B.300d.txt'\n",
    "android_corpus_path = '../android_dataset/corpus.tsv'\n",
    "ubuntu_corpus_path = '../ubuntu_dataset/text_tokenized.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch #:  1\n",
      "lstm multi-margin loss on batch: 0.10016369074583054\n",
      "Time on batch: 38.725939989089966\n",
      "Working on batch #:  2\n",
      "lstm multi-margin loss on batch: 0.07415125519037247\n",
      "Time on batch: 52.00062870979309\n",
      "Working on batch #:  3\n",
      "lstm multi-margin loss on batch: 0.049514882266521454\n",
      "Time on batch: 42.72429919242859\n",
      "Working on batch #:  4\n",
      "lstm multi-margin loss on batch: 0.03558170795440674\n",
      "Time on batch: 33.90351915359497\n",
      "Working on batch #:  5\n",
      "lstm multi-margin loss on batch: 0.027691852301359177\n",
      "Time on batch: 164.3411569595337\n",
      "Working on batch #:  6\n",
      "lstm multi-margin loss on batch: 0.016386093571782112\n",
      "Time on batch: 42.048569440841675\n",
      "Working on batch #:  7\n",
      "lstm multi-margin loss on batch: 0.013205495662987232\n",
      "Time on batch: 68.38670110702515\n",
      "Working on batch #:  8\n",
      "lstm multi-margin loss on batch: 0.012787408195436\n",
      "Time on batch: 35.018958568573\n",
      "Working on batch #:  9\n",
      "lstm multi-margin loss on batch: 0.018541747704148293\n",
      "Time on batch: 28.809173107147217\n",
      "Working on batch #:  10\n",
      "lstm multi-margin loss on batch: 0.01856575720012188\n",
      "Time on batch: 64.62780809402466\n",
      "Working on batch #:  11\n",
      "lstm multi-margin loss on batch: 0.03704453632235527\n",
      "Time on batch: 135.203857421875\n",
      "Working on batch #:  12\n",
      "lstm multi-margin loss on batch: 0.02015441469848156\n",
      "Time on batch: 37.76292586326599\n",
      "Working on batch #:  13\n",
      "lstm multi-margin loss on batch: 0.011766939423978329\n",
      "Time on batch: 35.696218490600586\n",
      "Working on batch #:  14\n",
      "lstm multi-margin loss on batch: 0.01364610530436039\n",
      "Time on batch: 38.57252860069275\n",
      "Working on batch #:  15\n",
      "lstm multi-margin loss on batch: 0.018678566440939903\n",
      "Time on batch: 35.09546184539795\n",
      "Working on batch #:  16\n",
      "lstm multi-margin loss on batch: 0.008381891064345837\n",
      "Time on batch: 42.25098633766174\n",
      "Working on batch #:  17\n",
      "lstm multi-margin loss on batch: 0.014996602199971676\n",
      "Time on batch: 31.41181468963623\n",
      "Working on batch #:  18\n",
      "lstm multi-margin loss on batch: 0.019020328298211098\n",
      "Time on batch: 23.762694120407104\n",
      "Working on batch #:  19\n",
      "lstm multi-margin loss on batch: 0.012086572125554085\n",
      "Time on batch: 29.72717833518982\n",
      "Working on batch #:  20\n",
      "lstm multi-margin loss on batch: 0.010830594226717949\n",
      "Time on batch: 32.49928426742554\n",
      "Working on batch #:  21\n",
      "lstm multi-margin loss on batch: 0.010817944072186947\n",
      "Time on batch: 28.350303649902344\n",
      "Working on batch #:  22\n",
      "lstm multi-margin loss on batch: 0.022982265800237656\n",
      "Time on batch: 27.32824730873108\n",
      "Working on batch #:  23\n",
      "lstm multi-margin loss on batch: 0.012854562141001225\n",
      "Time on batch: 47.480536222457886\n",
      "Working on batch #:  24\n",
      "lstm multi-margin loss on batch: 0.017541276291012764\n",
      "Time on batch: 30.827107667922974\n",
      "Working on batch #:  25\n",
      "lstm multi-margin loss on batch: 0.007624035235494375\n",
      "Time on batch: 29.949421405792236\n",
      "Working on batch #:  26\n",
      "lstm multi-margin loss on batch: 0.014519326388835907\n",
      "Time on batch: 31.72374677658081\n",
      "Working on batch #:  27\n",
      "lstm multi-margin loss on batch: 0.009458109736442566\n",
      "Time on batch: 24.741836547851562\n",
      "Working on batch #:  28\n",
      "lstm multi-margin loss on batch: 0.013608970679342747\n",
      "Time on batch: 40.886948108673096\n",
      "Working on batch #:  29\n",
      "lstm multi-margin loss on batch: 0.011791935190558434\n",
      "Time on batch: 31.955042839050293\n",
      "Working on batch #:  30\n",
      "lstm multi-margin loss on batch: 0.014079471118748188\n",
      "Time on batch: 60.12720060348511\n",
      "Working on batch #:  31\n",
      "lstm multi-margin loss on batch: 0.015873175114393234\n",
      "Time on batch: 33.494030237197876\n",
      "Working on batch #:  32\n",
      "lstm multi-margin loss on batch: 0.008793688379228115\n",
      "Time on batch: 25.670528411865234\n",
      "Working on batch #:  33\n",
      "lstm multi-margin loss on batch: 0.006901477929204702\n",
      "Time on batch: 31.782569408416748\n",
      "Working on batch #:  34\n",
      "lstm multi-margin loss on batch: 0.007745497394353151\n",
      "Time on batch: 32.88424468040466\n",
      "Working on batch #:  35\n",
      "lstm multi-margin loss on batch: 0.009192775003612041\n",
      "Time on batch: 25.62620258331299\n",
      "Working on batch #:  36\n",
      "lstm multi-margin loss on batch: 0.016077784821391106\n",
      "Time on batch: 29.07861828804016\n",
      "Working on batch #:  37\n",
      "lstm multi-margin loss on batch: 0.01271785981953144\n",
      "Time on batch: 33.42965650558472\n",
      "Working on batch #:  38\n",
      "lstm multi-margin loss on batch: 0.01578027941286564\n",
      "Time on batch: 27.925033569335938\n",
      "Working on batch #:  39\n",
      "lstm multi-margin loss on batch: 0.008588559925556183\n",
      "Time on batch: 47.32781481742859\n",
      "Working on batch #:  40\n",
      "lstm multi-margin loss on batch: 0.013760336674749851\n",
      "Time on batch: 29.084712982177734\n",
      "Working on batch #:  41\n",
      "lstm multi-margin loss on batch: 0.007038043346256018\n",
      "Time on batch: 28.544299840927124\n",
      "Working on batch #:  42\n",
      "lstm multi-margin loss on batch: 0.0049193184822797775\n",
      "Time on batch: 58.50257205963135\n",
      "Working on batch #:  43\n",
      "lstm multi-margin loss on batch: 0.01332507561892271\n",
      "Time on batch: 40.296204805374146\n",
      "Working on batch #:  44\n",
      "lstm multi-margin loss on batch: 0.021294398233294487\n",
      "Time on batch: 232.57093334197998\n",
      "Working on batch #:  45\n",
      "lstm multi-margin loss on batch: 0.008867690339684486\n",
      "Time on batch: 32.76855254173279\n",
      "Working on batch #:  46\n",
      "lstm multi-margin loss on batch: 0.0210095401853323\n",
      "Time on batch: 40.414306640625\n",
      "Working on batch #:  47\n",
      "lstm multi-margin loss on batch: 0.007342993281781673\n",
      "Time on batch: 41.64476752281189\n",
      "Working on batch #:  48\n",
      "lstm multi-margin loss on batch: 0.008761456236243248\n",
      "Time on batch: 46.15893793106079\n",
      "Working on batch #:  49\n",
      "lstm multi-margin loss on batch: 0.019221952185034752\n",
      "Time on batch: 42.94487476348877\n",
      "Working on batch #:  50\n",
      "lstm multi-margin loss on batch: 0.005672255530953407\n",
      "Time on batch: 33.56781005859375\n",
      "Working on batch #:  51\n",
      "lstm multi-margin loss on batch: 0.0183742493391037\n",
      "Time on batch: 30.13672685623169\n",
      "Working on batch #:  52\n",
      "lstm multi-margin loss on batch: 0.005702093243598938\n",
      "Time on batch: 44.81578707695007\n",
      "Working on batch #:  53\n",
      "lstm multi-margin loss on batch: 0.0087216692045331\n",
      "Time on batch: 26.40947651863098\n",
      "Working on batch #:  54\n",
      "lstm multi-margin loss on batch: 0.007755302358418703\n",
      "Time on batch: 42.475746631622314\n",
      "Working on batch #:  55\n",
      "lstm multi-margin loss on batch: 0.011614283546805382\n",
      "Time on batch: 45.6535530090332\n",
      "Working on batch #:  56\n",
      "lstm multi-margin loss on batch: 0.011349203996360302\n",
      "Time on batch: 28.12684154510498\n",
      "Working on batch #:  57\n",
      "lstm multi-margin loss on batch: 0.010844368487596512\n",
      "Time on batch: 28.43510937690735\n",
      "Working on batch #:  58\n",
      "lstm multi-margin loss on batch: 0.00853693950921297\n",
      "Time on batch: 31.954949140548706\n",
      "Working on batch #:  59\n",
      "lstm multi-margin loss on batch: 0.013710414990782738\n",
      "Time on batch: 33.799490213394165\n",
      "Working on batch #:  60\n",
      "lstm multi-margin loss on batch: 0.010692031122744083\n",
      "Time on batch: 40.38834738731384\n",
      "Working on batch #:  61\n",
      "lstm multi-margin loss on batch: 0.0071524325758218765\n",
      "Time on batch: 47.01832318305969\n",
      "Working on batch #:  62\n",
      "lstm multi-margin loss on batch: 0.007128147874027491\n",
      "Time on batch: 90.67860078811646\n",
      "Working on batch #:  63\n",
      "lstm multi-margin loss on batch: 0.025734255090355873\n",
      "Time on batch: 320.6560137271881\n",
      "Working on batch #:  64\n",
      "lstm multi-margin loss on batch: 0.017516516149044037\n",
      "Time on batch: 28.079614400863647\n",
      "Working on batch #:  65\n",
      "lstm multi-margin loss on batch: 0.006472005974501371\n",
      "Time on batch: 32.50557017326355\n",
      "Working on batch #:  66\n",
      "lstm multi-margin loss on batch: 0.011842616833746433\n",
      "Time on batch: 35.410523891448975\n",
      "Working on batch #:  67\n",
      "lstm multi-margin loss on batch: 0.016232917085289955\n",
      "Time on batch: 45.71697998046875\n",
      "Working on batch #:  68\n",
      "lstm multi-margin loss on batch: 0.007561995182186365\n",
      "Time on batch: 27.036311864852905\n",
      "Working on batch #:  69\n",
      "lstm multi-margin loss on batch: 0.007948236539959908\n",
      "Time on batch: 326.4456822872162\n",
      "Working on batch #:  70\n",
      "lstm multi-margin loss on batch: 0.015673726797103882\n",
      "Time on batch: 39.357970237731934\n",
      "Working on batch #:  71\n",
      "lstm multi-margin loss on batch: 0.008973786607384682\n",
      "Time on batch: 76.30727672576904\n",
      "Working on batch #:  72\n",
      "lstm multi-margin loss on batch: 0.017924191430211067\n",
      "Time on batch: 37.031723499298096\n",
      "Working on batch #:  73\n",
      "lstm multi-margin loss on batch: 0.009796533733606339\n",
      "Time on batch: 30.761683225631714\n",
      "Working on batch #:  74\n",
      "lstm multi-margin loss on batch: 0.00844514463096857\n",
      "Time on batch: 34.155070304870605\n",
      "Working on batch #:  75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm multi-margin loss on batch: 0.01374753937125206\n",
      "Time on batch: 32.91535711288452\n",
      "Working on batch #:  76\n",
      "lstm multi-margin loss on batch: 0.008533590473234653\n",
      "Time on batch: 27.039607524871826\n",
      "Working on batch #:  77\n",
      "lstm multi-margin loss on batch: 0.008358956314623356\n",
      "Time on batch: 24.920485734939575\n",
      "Working on batch #:  78\n",
      "lstm multi-margin loss on batch: 0.005405467934906483\n",
      "Time on batch: 25.544949769973755\n",
      "Working on batch #:  79\n",
      "lstm multi-margin loss on batch: 0.009258832782506943\n",
      "Time on batch: 38.40528082847595\n",
      "Working on batch #:  80\n",
      "lstm multi-margin loss on batch: 0.011072542518377304\n",
      "Time on batch: 31.269765615463257\n",
      "Working on batch #:  81\n",
      "lstm multi-margin loss on batch: 0.006201568059623241\n",
      "Time on batch: 37.517468214035034\n",
      "Working on batch #:  82\n",
      "lstm multi-margin loss on batch: 0.007232066243886948\n",
      "Time on batch: 27.48449444770813\n",
      "Working on batch #:  83\n",
      "lstm multi-margin loss on batch: 0.008635452948510647\n",
      "Time on batch: 37.25682091712952\n",
      "Working on batch #:  84\n",
      "lstm multi-margin loss on batch: 0.008951735682785511\n",
      "Time on batch: 28.996293306350708\n",
      "Working on batch #:  85\n",
      "lstm multi-margin loss on batch: 0.005597560200840235\n",
      "Time on batch: 28.537330627441406\n",
      "Working on batch #:  86\n",
      "lstm multi-margin loss on batch: 0.006493722554296255\n",
      "Time on batch: 38.69963312149048\n",
      "Working on batch #:  87\n",
      "lstm multi-margin loss on batch: 0.009433295577764511\n",
      "Time on batch: 34.35877323150635\n",
      "Working on batch #:  88\n",
      "lstm multi-margin loss on batch: 0.0043065352365374565\n",
      "Time on batch: 36.57735466957092\n",
      "Working on batch #:  89\n",
      "lstm multi-margin loss on batch: 0.010433932766318321\n",
      "Time on batch: 29.492544889450073\n",
      "Working on batch #:  90\n",
      "lstm multi-margin loss on batch: 0.005345461890101433\n",
      "Time on batch: 34.063029766082764\n",
      "Working on batch #:  91\n",
      "lstm multi-margin loss on batch: 0.011934122070670128\n",
      "Time on batch: 34.70958971977234\n",
      "Working on batch #:  92\n",
      "lstm multi-margin loss on batch: 0.009629720821976662\n",
      "Time on batch: 25.452478170394897\n",
      "Working on batch #:  94\n",
      "lstm multi-margin loss on batch: 0.004618819337338209\n",
      "Time on batch: 29.400409698486328\n",
      "Working on batch #:  95\n",
      "lstm multi-margin loss on batch: 0.007427001837641001\n",
      "Time on batch: 31.783873558044434\n",
      "Working on batch #:  96\n",
      "lstm multi-margin loss on batch: 0.008845796808600426\n",
      "Time on batch: 30.299570560455322\n",
      "Working on batch #:  97\n",
      "lstm multi-margin loss on batch: 0.007262950763106346\n",
      "Time on batch: 27.016626834869385\n",
      "Working on batch #:  98\n",
      "lstm multi-margin loss on batch: 0.004499654285609722\n",
      "Time on batch: 25.582814931869507\n",
      "Working on batch #:  99\n",
      "lstm multi-margin loss on batch: 0.016366418451070786\n",
      "Time on batch: 145.62716484069824\n",
      "Working on batch #:  100\n",
      "lstm multi-margin loss on batch: 0.01448318362236023\n",
      "Time on batch: 46.78162717819214\n",
      "119685\n",
      "0 2394 4788 7182 9576 11970 14364 16758 19152 21546 23940 26334 28728 31122 33516 35910 38304 40698 43092 45486 47880 50274 52668 55062 57456 59850 62244 64638 67032 69426 71820 74214 76608 79002 81396 83790 86184 88578 90972 93366 95760 98154 100548 102942 105336 107730 110124 112518 114912 117306 Dev AUC score: 0.56935018137\n",
      "Working on batch #:  101\n",
      "lstm multi-margin loss on batch: 0.007998393848538399\n",
      "Time on batch: 34.00970983505249\n",
      "Working on batch #:  102\n",
      "lstm multi-margin loss on batch: 0.008924659341573715\n",
      "Time on batch: 27.85650634765625\n",
      "Working on batch #:  103\n",
      "lstm multi-margin loss on batch: 0.006518154870718718\n",
      "Time on batch: 27.660220623016357\n",
      "Working on batch #:  104\n",
      "lstm multi-margin loss on batch: 0.008971314877271652\n",
      "Time on batch: 32.63728952407837\n",
      "Working on batch #:  105\n",
      "lstm multi-margin loss on batch: 0.00988825038075447\n",
      "Time on batch: 26.936208486557007\n",
      "Working on batch #:  106\n",
      "lstm multi-margin loss on batch: 0.009291674941778183\n",
      "Time on batch: 34.94802165031433\n",
      "Working on batch #:  107\n",
      "lstm multi-margin loss on batch: 0.010190793313086033\n",
      "Time on batch: 31.090553283691406\n",
      "Working on batch #:  108\n",
      "lstm multi-margin loss on batch: 0.009640201926231384\n",
      "Time on batch: 26.48212957382202\n",
      "Working on batch #:  109\n",
      "lstm multi-margin loss on batch: 0.009197148494422436\n",
      "Time on batch: 28.583765745162964\n",
      "Working on batch #:  110\n",
      "lstm multi-margin loss on batch: 0.011769444681704044\n",
      "Time on batch: 28.81080436706543\n",
      "Working on batch #:  111\n",
      "lstm multi-margin loss on batch: 0.010502832010388374\n",
      "Time on batch: 35.25701570510864\n",
      "Working on batch #:  112\n",
      "lstm multi-margin loss on batch: 0.013401150703430176\n",
      "Time on batch: 28.638558864593506\n",
      "Working on batch #:  113\n",
      "lstm multi-margin loss on batch: 0.0067610484547913074\n",
      "Time on batch: 35.56464958190918\n",
      "Working on batch #:  114\n",
      "lstm multi-margin loss on batch: 0.011445049196481705\n",
      "Time on batch: 34.129730224609375\n",
      "Working on batch #:  115\n",
      "lstm multi-margin loss on batch: 0.0066750445403158665\n",
      "Time on batch: 29.731115341186523\n",
      "Working on batch #:  116\n",
      "lstm multi-margin loss on batch: 0.009275066666305065\n",
      "Time on batch: 39.468923807144165\n",
      "Working on batch #:  117\n",
      "lstm multi-margin loss on batch: 0.008076359517872334\n",
      "Time on batch: 26.27552080154419\n",
      "Working on batch #:  118\n",
      "lstm multi-margin loss on batch: 0.012446564622223377\n",
      "Time on batch: 31.345550060272217\n",
      "Working on batch #:  119\n",
      "lstm multi-margin loss on batch: 0.003707791678607464\n",
      "Time on batch: 29.213708639144897\n",
      "Working on batch #:  120\n",
      "lstm multi-margin loss on batch: 0.00752111105248332\n",
      "Time on batch: 31.866557359695435\n",
      "Working on batch #:  121\n",
      "lstm multi-margin loss on batch: 0.005298036616295576\n",
      "Time on batch: 29.24090886116028\n",
      "Working on batch #:  122\n",
      "lstm multi-margin loss on batch: 0.015829479321837425\n",
      "Time on batch: 26.064293384552002\n",
      "Working on batch #:  123\n",
      "lstm multi-margin loss on batch: 0.009978009387850761\n",
      "Time on batch: 33.800862312316895\n",
      "Working on batch #:  124\n",
      "lstm multi-margin loss on batch: 0.01224147342145443\n",
      "Time on batch: 35.83528423309326\n",
      "Working on batch #:  125\n",
      "lstm multi-margin loss on batch: 0.01641649752855301\n",
      "Time on batch: 30.425626754760742\n",
      "Working on batch #:  126\n",
      "lstm multi-margin loss on batch: 0.004812989849597216\n",
      "Time on batch: 30.542828798294067\n",
      "Working on batch #:  127\n",
      "lstm multi-margin loss on batch: 0.010226085782051086\n",
      "Time on batch: 33.86402344703674\n",
      "Working on batch #:  128\n",
      "lstm multi-margin loss on batch: 0.010083936154842377\n",
      "Time on batch: 28.98430633544922\n",
      "Working on batch #:  129\n",
      "lstm multi-margin loss on batch: 0.006434289738535881\n",
      "Time on batch: 44.085660219192505\n",
      "Working on batch #:  130\n",
      "lstm multi-margin loss on batch: 0.006274093873798847\n",
      "Time on batch: 43.581857681274414\n",
      "Working on batch #:  131\n",
      "lstm multi-margin loss on batch: 0.013255287893116474\n",
      "Time on batch: 33.345399618148804\n",
      "Working on batch #:  132\n",
      "lstm multi-margin loss on batch: 0.008866336196660995\n",
      "Time on batch: 25.882513284683228\n",
      "Working on batch #:  133\n",
      "lstm multi-margin loss on batch: 0.011027279309928417\n",
      "Time on batch: 28.943716764450073\n",
      "Working on batch #:  134\n",
      "lstm multi-margin loss on batch: 0.01242431066930294\n",
      "Time on batch: 43.96847319602966\n",
      "Working on batch #:  135\n",
      "lstm multi-margin loss on batch: 0.005288281943649054\n",
      "Time on batch: 47.32641243934631\n",
      "Working on batch #:  136\n",
      "lstm multi-margin loss on batch: 0.00889746192842722\n",
      "Time on batch: 30.352038145065308\n",
      "Working on batch #:  137\n",
      "lstm multi-margin loss on batch: 0.011787288822233677\n",
      "Time on batch: 27.052321195602417\n",
      "Working on batch #:  138\n",
      "lstm multi-margin loss on batch: 0.013933619484305382\n",
      "Time on batch: 29.79928708076477\n",
      "Working on batch #:  139\n",
      "lstm multi-margin loss on batch: 0.009065165184438229\n",
      "Time on batch: 39.144883155822754\n",
      "Working on batch #:  140\n",
      "lstm multi-margin loss on batch: 0.007185667287558317\n",
      "Time on batch: 29.34300422668457\n",
      "Working on batch #:  141\n",
      "lstm multi-margin loss on batch: 0.01379688736051321\n",
      "Time on batch: 26.24382472038269\n",
      "Working on batch #:  142\n",
      "lstm multi-margin loss on batch: 0.005735498853027821\n",
      "Time on batch: 35.579559087753296\n",
      "Working on batch #:  143\n",
      "lstm multi-margin loss on batch: 0.006307678297162056\n",
      "Time on batch: 25.335830450057983\n",
      "Working on batch #:  144\n",
      "lstm multi-margin loss on batch: 0.007816465571522713\n",
      "Time on batch: 32.87582015991211\n",
      "Working on batch #:  145\n",
      "lstm multi-margin loss on batch: 0.009017647244036198\n",
      "Time on batch: 29.571739196777344\n",
      "Working on batch #:  146\n",
      "lstm multi-margin loss on batch: 0.00803382322192192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time on batch: 61.181384801864624\n",
      "Working on batch #:  147\n",
      "lstm multi-margin loss on batch: 0.005511469207704067\n",
      "Time on batch: 30.263870239257812\n",
      "Working on batch #:  148\n",
      "lstm multi-margin loss on batch: 0.005371310748159885\n",
      "Time on batch: 31.637228965759277\n",
      "Working on batch #:  149\n",
      "lstm multi-margin loss on batch: 0.011021154932677746\n",
      "Time on batch: 28.040550470352173\n",
      "Working on batch #:  150\n",
      "lstm multi-margin loss on batch: 0.006077388767153025\n",
      "Time on batch: 26.61675524711609\n",
      "Working on batch #:  151\n",
      "lstm multi-margin loss on batch: 0.007018269971013069\n",
      "Time on batch: 41.56206679344177\n",
      "Working on batch #:  152\n",
      "lstm multi-margin loss on batch: 0.014008154161274433\n",
      "Time on batch: 91.24815225601196\n",
      "Working on batch #:  153\n",
      "lstm multi-margin loss on batch: 0.013529484160244465\n",
      "Time on batch: 37.70327925682068\n",
      "Working on batch #:  154\n",
      "lstm multi-margin loss on batch: 0.010403303429484367\n",
      "Time on batch: 50.49574542045593\n",
      "Working on batch #:  155\n",
      "lstm multi-margin loss on batch: 0.008762318640947342\n",
      "Time on batch: 35.59313082695007\n",
      "Working on batch #:  156\n",
      "lstm multi-margin loss on batch: 0.010500027798116207\n",
      "Time on batch: 31.8902485370636\n",
      "Working on batch #:  157\n",
      "lstm multi-margin loss on batch: 0.010035530664026737\n",
      "Time on batch: 29.552166223526\n",
      "Working on batch #:  158\n",
      "lstm multi-margin loss on batch: 0.0057868813164532185\n",
      "Time on batch: 33.59116721153259\n",
      "Working on batch #:  159\n",
      "lstm multi-margin loss on batch: 0.00854917336255312\n",
      "Time on batch: 36.40970301628113\n",
      "Working on batch #:  160\n",
      "lstm multi-margin loss on batch: 0.011146248318254948\n",
      "Time on batch: 33.275734186172485\n",
      "Working on batch #:  161\n",
      "lstm multi-margin loss on batch: 0.008495165035128593\n",
      "Time on batch: 29.366178035736084\n",
      "Working on batch #:  162\n",
      "lstm multi-margin loss on batch: 0.004991716239601374\n",
      "Time on batch: 28.85324215888977\n",
      "Working on batch #:  163\n",
      "lstm multi-margin loss on batch: 0.0068014245480299\n",
      "Time on batch: 32.74035978317261\n",
      "Working on batch #:  164\n",
      "lstm multi-margin loss on batch: 0.007939700968563557\n",
      "Time on batch: 30.410037517547607\n",
      "Working on batch #:  165\n",
      "lstm multi-margin loss on batch: 0.014182083308696747\n",
      "Time on batch: 37.47743225097656\n",
      "Working on batch #:  166\n",
      "lstm multi-margin loss on batch: 0.007219761144369841\n",
      "Time on batch: 62.33127570152283\n",
      "Working on batch #:  167\n",
      "lstm multi-margin loss on batch: 0.007885772734880447\n",
      "Time on batch: 64.12710881233215\n",
      "Working on batch #:  168\n",
      "lstm multi-margin loss on batch: 0.01063638087362051\n",
      "Time on batch: 25.887959480285645\n",
      "Working on batch #:  169\n",
      "lstm multi-margin loss on batch: 0.005519431550055742\n",
      "Time on batch: 206.1857659816742\n",
      "Working on batch #:  170\n",
      "lstm multi-margin loss on batch: 0.0040164426900446415\n",
      "Time on batch: 43.87208390235901\n",
      "Working on batch #:  171\n",
      "lstm multi-margin loss on batch: 0.012170626781880856\n",
      "Time on batch: 42.39340257644653\n",
      "Working on batch #:  172\n",
      "lstm multi-margin loss on batch: 0.006204619538038969\n",
      "Time on batch: 29.150840282440186\n",
      "Working on batch #:  173\n",
      "lstm multi-margin loss on batch: 0.007158311083912849\n",
      "Time on batch: 36.79635548591614\n",
      "Working on batch #:  174\n",
      "lstm multi-margin loss on batch: 0.002996730152517557\n",
      "Time on batch: 25.360859632492065\n",
      "Working on batch #:  175\n",
      "lstm multi-margin loss on batch: 0.008474009111523628\n",
      "Time on batch: 30.55645513534546\n",
      "Working on batch #:  176\n",
      "lstm multi-margin loss on batch: 0.009616420604288578\n",
      "Time on batch: 31.381023168563843\n",
      "Working on batch #:  177\n",
      "lstm multi-margin loss on batch: 0.004736654460430145\n",
      "Time on batch: 30.05768918991089\n",
      "Working on batch #:  178\n",
      "lstm multi-margin loss on batch: 0.0023827417753636837\n",
      "Time on batch: 35.42521858215332\n",
      "Working on batch #:  179\n",
      "lstm multi-margin loss on batch: 0.006442556623369455\n",
      "Time on batch: 28.600066900253296\n",
      "Working on batch #:  180\n",
      "lstm multi-margin loss on batch: 0.007657101843506098\n",
      "Time on batch: 39.109551668167114\n",
      "Working on batch #:  181\n",
      "lstm multi-margin loss on batch: 0.008235357701778412\n",
      "Time on batch: 47.21017098426819\n",
      "Working on batch #:  182\n",
      "lstm multi-margin loss on batch: 0.00923062488436699\n",
      "Time on batch: 34.71865963935852\n",
      "Working on batch #:  183\n",
      "lstm multi-margin loss on batch: 0.005758578889071941\n",
      "Time on batch: 33.84792470932007\n",
      "Working on batch #:  184\n",
      "lstm multi-margin loss on batch: 0.00717943673953414\n",
      "Time on batch: 27.749488830566406\n",
      "Working on batch #:  185\n",
      "lstm multi-margin loss on batch: 0.0034024256747215986\n",
      "Time on batch: 34.42522573471069\n",
      "Working on batch #:  186\n",
      "lstm multi-margin loss on batch: 0.012205418199300766\n",
      "Time on batch: 29.324772834777832\n",
      "Working on batch #:  187\n",
      "lstm multi-margin loss on batch: 0.006910359486937523\n",
      "Time on batch: 30.593329906463623\n",
      "Working on batch #:  188\n",
      "lstm multi-margin loss on batch: 0.016811687499284744\n",
      "Time on batch: 27.491849422454834\n",
      "Working on batch #:  189\n",
      "lstm multi-margin loss on batch: 0.014791262336075306\n",
      "Time on batch: 44.478110790252686\n",
      "Working on batch #:  190\n",
      "lstm multi-margin loss on batch: 0.0040473248809576035\n",
      "Time on batch: 37.10306715965271\n",
      "Working on batch #:  191\n",
      "lstm multi-margin loss on batch: 0.006361257750540972\n",
      "Time on batch: 29.76494574546814\n",
      "Working on batch #:  192\n",
      "lstm multi-margin loss on batch: 0.003741980530321598\n",
      "Time on batch: 50.29927897453308\n",
      "Working on batch #:  193\n",
      "lstm multi-margin loss on batch: 0.011121773160994053\n",
      "Time on batch: 41.92313838005066\n",
      "Working on batch #:  194\n",
      "lstm multi-margin loss on batch: 0.010567616671323776\n",
      "Time on batch: 33.71738934516907\n",
      "Working on batch #:  195\n",
      "lstm multi-margin loss on batch: 0.0067662824876606464\n",
      "Time on batch: 29.87411379814148\n",
      "Working on batch #:  196\n",
      "lstm multi-margin loss on batch: 0.008481483906507492\n",
      "Time on batch: 57.75932788848877\n",
      "Working on batch #:  197\n",
      "lstm multi-margin loss on batch: 0.008459214121103287\n",
      "Time on batch: 25.21402931213379\n",
      "Working on batch #:  198\n",
      "lstm multi-margin loss on batch: 0.01184186153113842\n",
      "Time on batch: 32.60879850387573\n",
      "Working on batch #:  199\n",
      "lstm multi-margin loss on batch: 0.004686370026320219\n",
      "Time on batch: 32.59907245635986\n",
      "Working on batch #:  200\n",
      "lstm multi-margin loss on batch: 0.009667298756539822\n",
      "Time on batch: 59.72268867492676\n",
      "119685\n",
      "0 2394 4788 7182 9576 11970 14364 16758 19152 21546 23940 26334 28728 31122 33516 35910 38304 40698 43092 45486 47880 50274 52668 55062 57456 59850 62244 64638 67032 69426 71820 74214 76608 79002 81396 83790 86184 88578 90972 93366 95760 98154 100548 102942 105336 107730 110124 112518 114912 117306 Dev AUC score: 0.60783490621\n",
      "Working on batch #:  201\n",
      "lstm multi-margin loss on batch: 0.0035747173242270947\n",
      "Time on batch: 27.258206129074097\n",
      "Working on batch #:  202\n",
      "lstm multi-margin loss on batch: 0.008754833601415157\n",
      "Time on batch: 32.16273522377014\n",
      "Working on batch #:  203\n",
      "lstm multi-margin loss on batch: 0.008990275673568249\n",
      "Time on batch: 34.11286115646362\n",
      "Working on batch #:  204\n",
      "lstm multi-margin loss on batch: 0.008842908777296543\n",
      "Time on batch: 56.65649771690369\n",
      "Working on batch #:  205\n",
      "lstm multi-margin loss on batch: 0.006324356887489557\n",
      "Time on batch: 26.840585708618164\n",
      "Working on batch #:  206\n",
      "lstm multi-margin loss on batch: 0.01057110633701086\n",
      "Time on batch: 47.09782910346985\n",
      "Working on batch #:  207\n",
      "lstm multi-margin loss on batch: 0.008042586036026478\n",
      "Time on batch: 30.397806882858276\n",
      "Working on batch #:  208\n",
      "lstm multi-margin loss on batch: 0.007849880494177341\n",
      "Time on batch: 49.86149454116821\n",
      "Working on batch #:  209\n",
      "lstm multi-margin loss on batch: 0.004297223407775164\n",
      "Time on batch: 25.676400184631348\n",
      "Working on batch #:  210\n",
      "lstm multi-margin loss on batch: 0.00700830202549696\n",
      "Time on batch: 29.932534217834473\n",
      "Working on batch #:  211\n",
      "lstm multi-margin loss on batch: 0.007261295337229967\n",
      "Time on batch: 31.149938344955444\n",
      "Working on batch #:  212\n",
      "lstm multi-margin loss on batch: 0.0073505574837327\n",
      "Time on batch: 31.271409034729004\n",
      "Working on batch #:  213\n",
      "lstm multi-margin loss on batch: 0.013947135768830776\n",
      "Time on batch: 38.89184308052063\n",
      "Working on batch #:  214\n",
      "lstm multi-margin loss on batch: 0.007143914699554443\n",
      "Time on batch: 35.83937621116638\n",
      "Working on batch #:  215\n",
      "lstm multi-margin loss on batch: 0.0051805246621370316\n",
      "Time on batch: 29.188657522201538\n",
      "Working on batch #:  216\n",
      "lstm multi-margin loss on batch: 0.008999641053378582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time on batch: 35.289726972579956\n",
      "Working on batch #:  217\n",
      "lstm multi-margin loss on batch: 0.010398208163678646\n",
      "Time on batch: 26.398828983306885\n",
      "Working on batch #:  218\n",
      "lstm multi-margin loss on batch: 0.003970896825194359\n",
      "Time on batch: 29.9364914894104\n",
      "Working on batch #:  219\n",
      "lstm multi-margin loss on batch: 0.007909082807600498\n",
      "Time on batch: 31.946791172027588\n",
      "Working on batch #:  220\n",
      "lstm multi-margin loss on batch: 0.01221136562526226\n",
      "Time on batch: 197.52108597755432\n",
      "Working on batch #:  221\n",
      "lstm multi-margin loss on batch: 0.010511831380426884\n",
      "Time on batch: 56.519999980926514\n",
      "Working on batch #:  222\n",
      "lstm multi-margin loss on batch: 0.004416931886225939\n",
      "Time on batch: 32.648961544036865\n",
      "Working on batch #:  223\n",
      "lstm multi-margin loss on batch: 0.002608521841466427\n",
      "Time on batch: 27.58937406539917\n",
      "Working on batch #:  224\n",
      "lstm multi-margin loss on batch: 0.0053591299802064896\n",
      "Time on batch: 31.74249291419983\n",
      "Working on batch #:  225\n",
      "lstm multi-margin loss on batch: 0.020004073157906532\n",
      "Time on batch: 68.21630787849426\n",
      "Working on batch #:  226\n",
      "lstm multi-margin loss on batch: 0.009309404529631138\n",
      "Time on batch: 37.820682764053345\n",
      "Working on batch #:  227\n",
      "lstm multi-margin loss on batch: 0.011019494384527206\n",
      "Time on batch: 36.51615619659424\n",
      "Working on batch #:  228\n",
      "lstm multi-margin loss on batch: 0.008047026582062244\n",
      "Time on batch: 34.58322596549988\n",
      "Working on batch #:  229\n",
      "lstm multi-margin loss on batch: 0.0048678480088710785\n",
      "Time on batch: 30.388899326324463\n",
      "Working on batch #:  230\n",
      "lstm multi-margin loss on batch: 0.00848943181335926\n",
      "Time on batch: 38.30382537841797\n",
      "Working on batch #:  231\n",
      "lstm multi-margin loss on batch: 0.006299106869846582\n",
      "Time on batch: 35.86954212188721\n",
      "Working on batch #:  232\n",
      "lstm multi-margin loss on batch: 0.005686930380761623\n",
      "Time on batch: 198.49529337882996\n",
      "Working on batch #:  233\n",
      "lstm multi-margin loss on batch: 0.005851238965988159\n",
      "Time on batch: 28.788341999053955\n",
      "Working on batch #:  234\n",
      "lstm multi-margin loss on batch: 0.006867213640362024\n",
      "Time on batch: 32.57213377952576\n",
      "Working on batch #:  235\n",
      "lstm multi-margin loss on batch: 0.005364908371120691\n",
      "Time on batch: 29.277730464935303\n",
      "Working on batch #:  236\n",
      "lstm multi-margin loss on batch: 0.00533640431240201\n",
      "Time on batch: 27.99423909187317\n",
      "Working on batch #:  237\n",
      "lstm multi-margin loss on batch: 0.0047517637722194195\n",
      "Time on batch: 30.563634395599365\n",
      "Working on batch #:  238\n",
      "lstm multi-margin loss on batch: 0.006120940670371056\n",
      "Time on batch: 31.395517826080322\n",
      "Working on batch #:  239\n",
      "lstm multi-margin loss on batch: 0.010356675833463669\n",
      "Time on batch: 33.82397389411926\n",
      "Working on batch #:  240\n",
      "lstm multi-margin loss on batch: 0.007269404362887144\n",
      "Time on batch: 33.94041681289673\n",
      "Working on batch #:  241\n",
      "lstm multi-margin loss on batch: 0.007126178592443466\n",
      "Time on batch: 36.1749222278595\n",
      "Working on batch #:  242\n",
      "lstm multi-margin loss on batch: 0.005280189216136932\n",
      "Time on batch: 26.301626682281494\n",
      "Working on batch #:  243\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to reallocate 0GB. Buy new RAM! at D:\\Projects\\pytorch\\torch\\lib\\TH\\THGeneral.c:298",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4a8c5a58b03c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mids_this_batch_for_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_question_ids_ubuntu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         loss_batch_similarity = train_lstm_question_similarity(lstm, ids_this_batch_for_lstm,\n\u001b[1;32m---> 46\u001b[1;33m         training_data_ubuntu, word2vec, ubuntu_id_to_data_title, ubuntu_id_to_data_body, word_to_id_vocab, truncation_val_title, truncation_val_body)\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0moverall_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_batch_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4a8c5a58b03c>\u001b[0m in \u001b[0;36mtrain_lstm_question_similarity\u001b[1;34m(lstm, batch_ids, batch_data, word2vec, id2Data_title, id2Data_body, word_to_id_vocab, truncation_val_title, truncation_val_body)\u001b[0m\n\u001b[0;32m     20\u001b[0m         dict_sequence_lengths, num_differing_questions, word_to_id_vocab, truncation_val_title, truncation_val_body, candidates=True)\n\u001b[0;32m     21\u001b[0m     main_qs_tuples_matrix = construct_qs_matrix_training(batch_ids, lstm, h0, c0, word2vec, id2Data_title, id2Data_body,\n\u001b[1;32m---> 22\u001b[1;33m         dict_sequence_lengths, num_differing_questions, word_to_id_vocab, truncation_val_title, truncation_val_body, candidates=False)\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0msimilarity_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates_qs_tuples_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_qs_tuples_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Harvard Grad school\\2017-2018 Academic Year\\Fall 2017\\Advanced Natural Language Processing (6.864)\\6.864 Project\\6.864 Project Part 2 - Final (Git)\\Code\\preprocess_text_to_tensors_lstm.py\u001b[0m in \u001b[0;36mconstruct_qs_matrix_training\u001b[1;34m(q_ids_sequential, lstm, h0, c0, word2vec, id2Data_title, id2Data_body, dict_sequence_lengths, num_differing_questions, word_to_id_vocab, truncation_val_title, truncation_val_body, candidates)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[0msum_h_qs_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_hidden_title\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[0mqs_padded_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_matrix_list_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mqs_hidden_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_padded_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0msum_h_qs_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_hidden_body\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[0mmean_pooled_h_qs_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_h_qs_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_seq_length_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         )\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, weight, hidden)\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mnexth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[0mhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;31m# hack to handle LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[1;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mgates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mingate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36maddmm\u001b[1;34m(cls, *args)\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36m_blas\u001b[1;34m(cls, args, inplace)\u001b[0m\n\u001b[0;32m    918\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\_functions\\blas.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[1;32m---> 26\u001b[1;33m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to reallocate 0GB. Buy new RAM! at D:\\Projects\\pytorch\\torch\\lib\\TH\\THGeneral.c:298"
     ]
    }
   ],
   "source": [
    "''' Encoder (LSTM) '''\n",
    "lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "loss_function_lstm = torch.nn.MultiMarginLoss(margin=margin)\n",
    "optimizer_lstm = torch.optim.Adam(lstm.parameters(), lr=lr_lstm, weight_decay = 0.00001)\n",
    "\n",
    "h0 = Variable(torch.zeros(first_dim, 1, hidden_size), requires_grad=False)\n",
    "c0 = Variable(torch.zeros(first_dim, 1, hidden_size), requires_grad=False)\n",
    "\n",
    "\n",
    "''' Procedural parameters '''\n",
    "num_batches = round(len(training_question_ids_ubuntu) / batch_size)\n",
    "auc_scorer = AUCMeter()\n",
    "\n",
    "\n",
    "def train_lstm_question_similarity(lstm, batch_ids, batch_data, word2vec, id2Data_title, id2Data_body, word_to_id_vocab, truncation_val_title, truncation_val_body):\n",
    "    lstm.train()\n",
    "    sequence_ids, dict_sequence_lengths = organize_ids_training(batch_ids, batch_data, num_differing_questions)\n",
    "\n",
    "    candidates_qs_tuples_matrix = construct_qs_matrix_training(sequence_ids, lstm, h0, c0, word2vec, id2Data_title, id2Data_body,\n",
    "        dict_sequence_lengths, num_differing_questions, word_to_id_vocab, truncation_val_title, truncation_val_body, candidates=True)\n",
    "    main_qs_tuples_matrix = construct_qs_matrix_training(batch_ids, lstm, h0, c0, word2vec, id2Data_title, id2Data_body,\n",
    "        dict_sequence_lengths, num_differing_questions, word_to_id_vocab, truncation_val_title, truncation_val_body, candidates=False)\n",
    "\n",
    "    similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-6)\n",
    "    target = Variable(torch.LongTensor([0] * int(len(sequence_ids) / (1 + num_differing_questions))))\n",
    "    loss_batch = loss_function_lstm(similarity_matrix, target)\n",
    "\n",
    "    print(\"lstm multi-margin loss on batch:\", loss_batch.data[0])\n",
    "    return loss_batch\n",
    "\n",
    "\n",
    "'''Begin training'''\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Train on whole training data set\n",
    "    for batch in range(1, num_batches + 1):\n",
    "        if batch == 93 or batch == 301:\n",
    "            continue\n",
    "        start = time.time()\n",
    "        optimizer_lstm.zero_grad()\n",
    "        print(\"Working on batch #: \", batch)\n",
    "\n",
    "        # Train on ubuntu similar question retrieval\n",
    "        ids_this_batch_for_lstm = training_question_ids_ubuntu[batch_size * (batch - 1):batch_size * batch]\n",
    "        loss_batch_similarity = train_lstm_question_similarity(lstm, ids_this_batch_for_lstm,\n",
    "        training_data_ubuntu, word2vec, ubuntu_id_to_data_title, ubuntu_id_to_data_body, word_to_id_vocab, truncation_val_title, truncation_val_body)\n",
    "\n",
    "        overall_loss = loss_batch_similarity\n",
    "        overall_loss.backward()\n",
    "        optimizer_lstm.step()\n",
    "\n",
    "        print(\"Time on batch:\", time.time() - start)\n",
    "        \n",
    "        if batch == 1 or batch % 100 == 0:\n",
    "            # Save model for this epoch\n",
    "            torch.save(lstm, '../Pickle/' + saved_model_name + '_e' + str(epoch) + '_b' + str(batch) + '.pth')\n",
    "\n",
    "            # Save optimizer for this epoch\n",
    "            torch.save(optimizer_lstm, '../Pickle/' + 'optim_lstm_direct_transfer' + '_e' + str(epoch) + '_b' + str(batch) + '.pth')\n",
    "            \n",
    "        if batch % 100 == 0:\n",
    "            # Evaluate on dev set for AUC score\n",
    "            dev_AUC_score = eval_model(lstm, dev_question_ids_android, dev_data_android, word2vec, android_id_to_data_title, android_id_to_data_body,\n",
    "                            word_to_id_vocab, truncation_val_title, truncation_val_body)\n",
    "\n",
    "            print(\"Dev AUC score:\", dev_AUC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119685\n",
      "0 4787 9574 14361 19148 23935 28722 33509 38296 43083 47870 52657 57444 62231 67018 71805 76592 81379 86166 90953 95740 100527 105314 110101 114888 119675 Test AUC score: 0.554701771596\n"
     ]
    }
   ],
   "source": [
    "''' Encoder (LSTM) '''\n",
    "#lstm_x = torch.load('../Pickle/lstm_dir_trans_title_body_p1_epoch1_batch200.pt')\n",
    "\n",
    "''' Procedural parameters '''\n",
    "auc_scorer = AUCMeter()\n",
    "\n",
    "# Evaluate on dev set for AUC score\n",
    "test_AUC_score = eval_model(lstm, test_question_ids_android, test_data_android, word2vec, android_id_to_data_title, android_id_to_data_body,\n",
    "                            word_to_id_vocab, truncation_val_title, truncation_val_body)\n",
    "\n",
    "print(\"Test AUC score:\", test_AUC_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
