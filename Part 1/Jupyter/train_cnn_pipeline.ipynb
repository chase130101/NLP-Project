{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from scoring_metrics import *\n",
    "from cnn_utils import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "\n",
    "saved_model_name = \"best_cnn_title_body\"\n",
    "\n",
    "'''Hyperparams dashboard'''\n",
    "margin = 0.3\n",
    "lr = 10**-3\n",
    "truncation_val_title = 40\n",
    "truncation_val_body = 60\n",
    "\n",
    "\n",
    "''' Data Prep '''\n",
    "word2vec = get_words_and_embeddings()\n",
    "id2Data = questionID_to_questionData_truncate(truncation_val_title, truncation_val_body)\n",
    "\n",
    "training_data = training_id_to_similar_different()\n",
    "trainingQuestionIds = list(training_data.keys())\n",
    "\n",
    "dev_data = devTest_id_to_similar_different(dev=True)\n",
    "dev_question_ids = list(dev_data.keys())\n",
    "\n",
    "test_data = devTest_id_to_similar_different(dev=False)\n",
    "test_question_ids = list(test_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch #:  1\n",
      "loss_on_batch: 0.012950340285897255  time_on_batch: 21.740819215774536\n",
      "MRR score on dev set: 0.6679373215818497\n",
      "MRR score on test set: 0.6643294471155751\n",
      "MAP score on dev set: 0.536302686749\n",
      "MAP score on test set: 0.543962369513\n",
      "Precision at 1 score on dev set: 0.515\n",
      "Precision at 1 score on test set: 0.475\n",
      "Precision at 5 score on dev set: 0.3880000000000002\n",
      "Precision at 5 score on test set: 0.37700000000000017\n",
      "Working on batch #:  2\n",
      "loss_on_batch: 0.028630465269088745  time_on_batch: 13.067741870880127\n",
      "Working on batch #:  3\n",
      "loss_on_batch: 0.03022531047463417  time_on_batch: 32.06428074836731\n",
      "Working on batch #:  4\n",
      "loss_on_batch: 0.01395374070852995  time_on_batch: 12.967473030090332\n",
      "Working on batch #:  5\n",
      "loss_on_batch: 0.021925244480371475  time_on_batch: 16.046658515930176\n",
      "Working on batch #:  6\n",
      "loss_on_batch: 0.02627919428050518  time_on_batch: 22.281232595443726\n",
      "Working on batch #:  7\n",
      "loss_on_batch: 0.030992334708571434  time_on_batch: 12.543344259262085\n",
      "Working on batch #:  8\n",
      "loss_on_batch: 0.02535982057452202  time_on_batch: 18.53928518295288\n",
      "Working on batch #:  9\n",
      "loss_on_batch: 0.054264962673187256  time_on_batch: 102.13665795326233\n",
      "Working on batch #:  10\n",
      "loss_on_batch: 0.022124391049146652  time_on_batch: 17.225793600082397\n",
      "Working on batch #:  11\n",
      "loss_on_batch: 0.016742561012506485  time_on_batch: 12.269621133804321\n",
      "Working on batch #:  12\n",
      "loss_on_batch: 0.015931755304336548  time_on_batch: 26.010143995285034\n",
      "Working on batch #:  13\n",
      "loss_on_batch: 0.024812746793031693  time_on_batch: 11.742215156555176\n",
      "Working on batch #:  14\n",
      "loss_on_batch: 0.031733907759189606  time_on_batch: 52.34715819358826\n",
      "Working on batch #:  15\n",
      "loss_on_batch: 0.024639029055833817  time_on_batch: 16.98214554786682\n",
      "Working on batch #:  16\n",
      "loss_on_batch: 0.01609317772090435  time_on_batch: 14.773273706436157\n",
      "Working on batch #:  17\n",
      "loss_on_batch: 0.009403889067471027  time_on_batch: 12.452109813690186\n",
      "Working on batch #:  18\n",
      "loss_on_batch: 0.033808112144470215  time_on_batch: 13.208114385604858\n",
      "Working on batch #:  19\n",
      "loss_on_batch: 0.014301623217761517  time_on_batch: 36.659454584121704\n",
      "Working on batch #:  20\n",
      "loss_on_batch: 0.019065411761403084  time_on_batch: 21.201362133026123\n",
      "Working on batch #:  21\n",
      "loss_on_batch: 0.02827995829284191  time_on_batch: 24.35474395751953\n",
      "Working on batch #:  22\n",
      "loss_on_batch: 0.0615670271217823  time_on_batch: 79.4612786769867\n",
      "Working on batch #:  23\n",
      "loss_on_batch: 0.023281194269657135  time_on_batch: 13.878897666931152\n",
      "Working on batch #:  24\n",
      "loss_on_batch: 0.011569865979254246  time_on_batch: 16.976129293441772\n",
      "Working on batch #:  25\n",
      "loss_on_batch: 0.01939559355378151  time_on_batch: 13.655299186706543\n",
      "Working on batch #:  26\n",
      "loss_on_batch: 0.02169474959373474  time_on_batch: 19.027583599090576\n",
      "Working on batch #:  27\n",
      "loss_on_batch: 0.019063621759414673  time_on_batch: 16.688364267349243\n",
      "Working on batch #:  28\n",
      "loss_on_batch: 0.025196997448801994  time_on_batch: 13.896941184997559\n",
      "Working on batch #:  29\n",
      "loss_on_batch: 0.02364104613661766  time_on_batch: 18.65459132194519\n",
      "Working on batch #:  30\n",
      "loss_on_batch: 0.018190152943134308  time_on_batch: 13.508912324905396\n",
      "Working on batch #:  31\n",
      "loss_on_batch: 0.018132995814085007  time_on_batch: 17.920639991760254\n",
      "Working on batch #:  32\n",
      "loss_on_batch: 0.02203863114118576  time_on_batch: 21.77488613128662\n",
      "Working on batch #:  33\n",
      "loss_on_batch: 0.013484032824635506  time_on_batch: 16.972121000289917\n",
      "Working on batch #:  34\n",
      "loss_on_batch: 0.02561597153544426  time_on_batch: 12.014942407608032\n",
      "Working on batch #:  35\n",
      "loss_on_batch: 0.022818533703684807  time_on_batch: 10.559071063995361\n",
      "Working on batch #:  36\n",
      "loss_on_batch: 0.022148489952087402  time_on_batch: 10.948104619979858\n",
      "Working on batch #:  37\n",
      "loss_on_batch: 0.024832647293806076  time_on_batch: 14.846466541290283\n",
      "Working on batch #:  38\n",
      "loss_on_batch: 0.019046982750296593  time_on_batch: 12.481181859970093\n",
      "Working on batch #:  39\n",
      "loss_on_batch: 0.014785408042371273  time_on_batch: 11.608861207962036\n",
      "Working on batch #:  40\n",
      "loss_on_batch: 0.03521353006362915  time_on_batch: 17.766232013702393\n",
      "Working on batch #:  41\n",
      "loss_on_batch: 0.02421892248094082  time_on_batch: 13.119883060455322\n",
      "Working on batch #:  42\n",
      "loss_on_batch: 0.01921885460615158  time_on_batch: 11.976837635040283\n",
      "Working on batch #:  43\n",
      "loss_on_batch: 0.029207300394773483  time_on_batch: 10.78166127204895\n",
      "Working on batch #:  44\n",
      "loss_on_batch: 0.019167542457580566  time_on_batch: 14.588786602020264\n",
      "Working on batch #:  45\n",
      "loss_on_batch: 0.017502523958683014  time_on_batch: 12.11821460723877\n",
      "Working on batch #:  46\n",
      "loss_on_batch: 0.03809691593050957  time_on_batch: 36.408790826797485\n",
      "Working on batch #:  47\n",
      "loss_on_batch: 0.017782393842935562  time_on_batch: 15.154286623001099\n",
      "Working on batch #:  48\n",
      "loss_on_batch: 0.029822388663887978  time_on_batch: 13.094811201095581\n",
      "Working on batch #:  49\n",
      "loss_on_batch: 0.015084005892276764  time_on_batch: 13.347485065460205\n",
      "Working on batch #:  50\n",
      "loss_on_batch: 0.011420978233218193  time_on_batch: 14.890582799911499\n",
      "MRR score on dev set: 0.651097246606854\n",
      "MRR score on test set: 0.6749444295047512\n",
      "MAP score on dev set: 0.532592846327\n",
      "MAP score on test set: 0.547523129152\n",
      "Precision at 1 score on dev set: 0.48\n",
      "Precision at 1 score on test set: 0.49\n",
      "Precision at 5 score on dev set: 0.3930000000000001\n",
      "Precision at 5 score on test set: 0.3910000000000003\n",
      "Working on batch #:  51\n",
      "loss_on_batch: 0.023782312870025635  time_on_batch: 16.15695071220398\n",
      "Working on batch #:  52\n",
      "loss_on_batch: 0.021959444507956505  time_on_batch: 13.0787672996521\n",
      "Working on batch #:  53\n",
      "loss_on_batch: 0.016458621248602867  time_on_batch: 10.933062553405762\n",
      "Working on batch #:  54\n",
      "loss_on_batch: 0.020932989194989204  time_on_batch: 11.852509021759033\n",
      "Working on batch #:  55\n",
      "loss_on_batch: 0.028833920136094093  time_on_batch: 15.125209093093872\n",
      "Working on batch #:  56\n",
      "loss_on_batch: 0.023204268887639046  time_on_batch: 23.99030089378357\n",
      "Working on batch #:  57\n",
      "loss_on_batch: 0.03328067064285278  time_on_batch: 14.366206407546997\n",
      "Working on batch #:  58\n",
      "loss_on_batch: 0.017422614619135857  time_on_batch: 16.98816180229187\n",
      "Working on batch #:  59\n",
      "loss_on_batch: 0.025806982070207596  time_on_batch: 33.508076667785645\n",
      "Working on batch #:  60\n",
      "loss_on_batch: 0.024608563631772995  time_on_batch: 22.91892719268799\n",
      "Working on batch #:  61\n",
      "loss_on_batch: 0.018189096823334694  time_on_batch: 15.947394609451294\n",
      "Working on batch #:  62\n",
      "loss_on_batch: 0.021866491064429283  time_on_batch: 15.622530698776245\n",
      "Working on batch #:  63\n",
      "loss_on_batch: 0.009084014222025871  time_on_batch: 11.687067747116089\n",
      "Working on batch #:  64\n",
      "loss_on_batch: 0.012626633048057556  time_on_batch: 11.56073260307312\n",
      "Working on batch #:  65\n",
      "loss_on_batch: 0.01290054526180029  time_on_batch: 12.057055234909058\n",
      "Working on batch #:  66\n",
      "loss_on_batch: 0.010846900753676891  time_on_batch: 16.186029195785522\n",
      "Working on batch #:  67\n",
      "loss_on_batch: 0.02727140113711357  time_on_batch: 11.584797382354736\n",
      "Working on batch #:  68\n",
      "loss_on_batch: 0.027354398742318153  time_on_batch: 19.573034524917603\n",
      "Working on batch #:  69\n",
      "loss_on_batch: 0.024163508787751198  time_on_batch: 12.249563694000244\n",
      "Working on batch #:  70\n",
      "loss_on_batch: 0.018678344786167145  time_on_batch: 10.955123901367188\n",
      "Working on batch #:  71\n",
      "loss_on_batch: 0.032442111521959305  time_on_batch: 14.283973932266235\n",
      "Working on batch #:  72\n",
      "loss_on_batch: 0.027500493451952934  time_on_batch: 12.781980752944946\n",
      "Working on batch #:  73\n",
      "loss_on_batch: 0.02267349883913994  time_on_batch: 18.038955688476562\n",
      "Working on batch #:  74\n",
      "loss_on_batch: 0.019437672570347786  time_on_batch: 14.422340154647827\n",
      "Working on batch #:  75\n",
      "loss_on_batch: 0.020436439663171768  time_on_batch: 11.712135076522827\n",
      "Working on batch #:  76\n",
      "loss_on_batch: 0.017710044980049133  time_on_batch: 13.535983562469482\n",
      "Working on batch #:  77\n",
      "loss_on_batch: 0.03364873304963112  time_on_batch: 25.830379724502563\n",
      "Working on batch #:  78\n",
      "loss_on_batch: 0.01343539822846651  time_on_batch: 16.426669359207153\n",
      "Working on batch #:  79\n",
      "loss_on_batch: 0.010082358494400978  time_on_batch: 16.81469988822937\n",
      "Working on batch #:  80\n",
      "loss_on_batch: 0.009436586871743202  time_on_batch: 10.170036792755127\n",
      "Working on batch #:  81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_on_batch: 0.010863441973924637  time_on_batch: 13.13291335105896\n",
      "Working on batch #:  82\n",
      "loss_on_batch: 0.029183262959122658  time_on_batch: 12.078110218048096\n",
      "Working on batch #:  83\n",
      "loss_on_batch: 0.0250753965228796  time_on_batch: 45.45584154129028\n",
      "Working on batch #:  84\n",
      "loss_on_batch: 0.02293611504137516  time_on_batch: 13.958107233047485\n",
      "Working on batch #:  85\n",
      "loss_on_batch: 0.02669551968574524  time_on_batch: 22.289252758026123\n",
      "Working on batch #:  86\n",
      "loss_on_batch: 0.021008215844631195  time_on_batch: 16.70139980316162\n",
      "Working on batch #:  87\n",
      "loss_on_batch: 0.058786336332559586  time_on_batch: 156.31770634651184\n",
      "Working on batch #:  88\n",
      "loss_on_batch: 0.014715636149048805  time_on_batch: 15.336303234100342\n",
      "Working on batch #:  89\n",
      "loss_on_batch: 0.009951694868505001  time_on_batch: 14.569249391555786\n",
      "Working on batch #:  90\n",
      "loss_on_batch: 0.02232053503394127  time_on_batch: 12.438554286956787\n",
      "Working on batch #:  91\n",
      "loss_on_batch: 0.014329861849546432  time_on_batch: 13.88730001449585\n",
      "Working on batch #:  92\n",
      "loss_on_batch: 0.021870026364922523  time_on_batch: 22.111165046691895\n",
      "Working on batch #:  93\n",
      "loss_on_batch: 0.019059428945183754  time_on_batch: 16.248842239379883\n",
      "Working on batch #:  94\n",
      "loss_on_batch: 0.024709124118089676  time_on_batch: 22.798327684402466\n",
      "Working on batch #:  95\n",
      "loss_on_batch: 0.018838534131646156  time_on_batch: 13.004567384719849\n",
      "Working on batch #:  96\n",
      "loss_on_batch: 0.03542089834809303  time_on_batch: 27.529181957244873\n",
      "Working on batch #:  97\n",
      "loss_on_batch: 0.011673911474645138  time_on_batch: 14.08344054222107\n",
      "Working on batch #:  98\n",
      "loss_on_batch: 0.04519715532660484  time_on_batch: 24.979405164718628\n",
      "Working on batch #:  99\n",
      "loss_on_batch: 0.014260645024478436  time_on_batch: 20.093416452407837\n",
      "Working on batch #:  100\n",
      "loss_on_batch: 0.011418412439525127  time_on_batch: 12.3999662399292\n",
      "MRR score on dev set: 0.6683715382625649\n",
      "MRR score on test set: 0.6791097114183352\n",
      "MAP score on dev set: 0.530115525728\n",
      "MAP score on test set: 0.559582276221\n",
      "Precision at 1 score on dev set: 0.495\n",
      "Precision at 1 score on test set: 0.495\n",
      "Precision at 5 score on dev set: 0.401\n",
      "Precision at 5 score on test set: 0.38800000000000023\n",
      "Working on batch #:  101\n",
      "loss_on_batch: 0.018732789903879166  time_on_batch: 13.39962387084961\n",
      "Working on batch #:  102\n",
      "loss_on_batch: 0.02402387000620365  time_on_batch: 15.583428621292114\n",
      "Working on batch #:  103\n",
      "loss_on_batch: 0.026333371177315712  time_on_batch: 16.928003549575806\n",
      "Working on batch #:  104\n",
      "loss_on_batch: 0.014915991574525833  time_on_batch: 31.854682445526123\n",
      "Working on batch #:  105\n",
      "loss_on_batch: 0.017887365072965622  time_on_batch: 17.00054121017456\n",
      "Working on batch #:  106\n",
      "loss_on_batch: 0.017724210396409035  time_on_batch: 14.868526458740234\n",
      "Working on batch #:  107\n",
      "loss_on_batch: 0.0164883304387331  time_on_batch: 17.517568588256836\n",
      "Working on batch #:  108\n",
      "loss_on_batch: 0.0167486984282732  time_on_batch: 22.16311764717102\n",
      "Working on batch #:  109\n",
      "loss_on_batch: 0.024185387417674065  time_on_batch: 12.383919715881348\n",
      "Working on batch #:  110\n",
      "loss_on_batch: 0.022184167057275772  time_on_batch: 31.168922424316406\n",
      "Working on batch #:  111\n",
      "loss_on_batch: 0.014451222494244576  time_on_batch: 14.447407484054565\n",
      "Working on batch #:  112\n",
      "loss_on_batch: 0.027191227301955223  time_on_batch: 11.747232913970947\n",
      "Working on batch #:  113\n",
      "loss_on_batch: 0.020256107673048973  time_on_batch: 13.176027297973633\n",
      "Working on batch #:  114\n",
      "loss_on_batch: 0.011959695257246494  time_on_batch: 12.081117391586304\n",
      "Working on batch #:  115\n",
      "loss_on_batch: 0.02185123972594738  time_on_batch: 15.222477912902832\n",
      "Working on batch #:  116\n",
      "loss_on_batch: 0.019019456580281258  time_on_batch: 14.673006772994995\n",
      "Working on batch #:  117\n",
      "loss_on_batch: 0.023863952606916428  time_on_batch: 11.380248308181763\n",
      "Working on batch #:  118\n",
      "loss_on_batch: 0.03591383993625641  time_on_batch: 20.12149167060852\n",
      "Working on batch #:  119\n",
      "loss_on_batch: 0.037645213305950165  time_on_batch: 21.439995288848877\n",
      "Working on batch #:  120\n",
      "loss_on_batch: 0.012566574849188328  time_on_batch: 14.685038805007935\n",
      "Working on batch #:  121\n",
      "loss_on_batch: 0.012308786623179913  time_on_batch: 14.626884698867798\n",
      "Working on batch #:  122\n",
      "loss_on_batch: 0.02139107696712017  time_on_batch: 28.787526845932007\n",
      "Working on batch #:  123\n",
      "loss_on_batch: 0.038711752742528915  time_on_batch: 84.26651930809021\n",
      "Working on batch #:  124\n",
      "loss_on_batch: 0.01709073968231678  time_on_batch: 11.03333044052124\n",
      "Working on batch #:  125\n",
      "loss_on_batch: 0.019178813323378563  time_on_batch: 18.958908081054688\n",
      "Working on batch #:  126\n",
      "loss_on_batch: 0.1400933712720871  time_on_batch: 179.79468846321106\n",
      "Working on batch #:  127\n",
      "loss_on_batch: 0.020617445930838585  time_on_batch: 12.81907844543457\n",
      "Working on batch #:  128\n",
      "loss_on_batch: 0.025817394256591797  time_on_batch: 12.944411754608154\n",
      "Working on batch #:  129\n",
      "loss_on_batch: 0.023284094408154488  time_on_batch: 12.740870952606201\n",
      "Working on batch #:  130\n",
      "loss_on_batch: 0.014672975987195969  time_on_batch: 18.968952894210815\n",
      "Working on batch #:  131\n",
      "loss_on_batch: 0.030221501365303993  time_on_batch: 17.513469696044922\n",
      "Working on batch #:  132\n",
      "loss_on_batch: 0.024315791204571724  time_on_batch: 16.946051120758057\n",
      "Working on batch #:  133\n",
      "loss_on_batch: 0.024004284292459488  time_on_batch: 22.9058940410614\n",
      "Working on batch #:  134\n",
      "loss_on_batch: 0.026538634672760963  time_on_batch: 18.467092990875244\n",
      "Working on batch #:  135\n",
      "loss_on_batch: 0.008292721584439278  time_on_batch: 15.077377796173096\n",
      "Working on batch #:  136\n",
      "loss_on_batch: 0.027126409113407135  time_on_batch: 10.960134267807007\n",
      "Working on batch #:  137\n",
      "loss_on_batch: 0.028567662462592125  time_on_batch: 12.263601303100586\n",
      "Working on batch #:  138\n",
      "loss_on_batch: 0.02303919941186905  time_on_batch: 230.7177128791809\n",
      "Working on batch #:  139\n",
      "loss_on_batch: 0.048587359488010406  time_on_batch: 13.132715225219727\n",
      "Working on batch #:  140\n",
      "loss_on_batch: 0.011924865655601025  time_on_batch: 13.604164838790894\n",
      "Working on batch #:  141\n",
      "loss_on_batch: 0.04132917895913124  time_on_batch: 50.67872357368469\n",
      "Working on batch #:  142\n",
      "loss_on_batch: 0.02189008705317974  time_on_batch: 13.494876623153687\n",
      "Working on batch #:  143\n",
      "loss_on_batch: 0.04696647822856903  time_on_batch: 19.413416147232056\n",
      "Working on batch #:  144\n",
      "loss_on_batch: 0.016528543084859848  time_on_batch: 15.29666543006897\n",
      "Working on batch #:  145\n",
      "loss_on_batch: 0.021904945373535156  time_on_batch: 13.352495670318604\n",
      "Working on batch #:  146\n",
      "loss_on_batch: 0.01650887541472912  time_on_batch: 12.257583618164062\n",
      "Working on batch #:  147\n",
      "loss_on_batch: 0.023791370913386345  time_on_batch: 17.195712566375732\n",
      "Working on batch #:  148\n",
      "loss_on_batch: 0.019093584269285202  time_on_batch: 18.193366289138794\n",
      "Working on batch #:  149\n",
      "loss_on_batch: 0.01806960254907608  time_on_batch: 13.743221282958984\n",
      "Working on batch #:  150\n",
      "loss_on_batch: 0.010733273811638355  time_on_batch: 19.0977725982666\n",
      "MRR score on dev set: 0.6446298897426715\n",
      "MRR score on test set: 0.6669917962468894\n",
      "MAP score on dev set: 0.527695415419\n",
      "MAP score on test set: 0.545109921529\n",
      "Precision at 1 score on dev set: 0.465\n",
      "Precision at 1 score on test set: 0.485\n",
      "Precision at 5 score on dev set: 0.41000000000000014\n",
      "Precision at 5 score on test set: 0.3880000000000001\n",
      "Working on batch #:  151\n",
      "loss_on_batch: 0.022386478260159492  time_on_batch: 14.610841274261475\n",
      "Working on batch #:  152\n",
      "loss_on_batch: 0.012126713059842587  time_on_batch: 13.067742586135864\n",
      "Working on batch #:  153\n",
      "loss_on_batch: 0.012943374924361706  time_on_batch: 12.321754217147827\n",
      "Working on batch #:  154\n",
      "loss_on_batch: 0.02673984318971634  time_on_batch: 12.609523296356201\n",
      "Working on batch #:  155\n",
      "loss_on_batch: 0.01411024946719408  time_on_batch: 11.952368021011353\n",
      "Working on batch #:  156\n",
      "loss_on_batch: 0.019488008692860603  time_on_batch: 12.514267206192017\n",
      "Working on batch #:  157\n",
      "loss_on_batch: 0.02550184540450573  time_on_batch: 18.538281679153442\n",
      "Working on batch #:  158\n",
      "loss_on_batch: 0.014447842724621296  time_on_batch: 20.170621395111084\n",
      "Working on batch #:  159\n",
      "loss_on_batch: 0.01846707984805107  time_on_batch: 19.198035955429077\n",
      "Working on batch #:  160\n",
      "loss_on_batch: 0.03212753310799599  time_on_batch: 12.049030780792236\n",
      "Working on batch #:  161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_on_batch: 0.00857936404645443  time_on_batch: 13.25423550605774\n",
      "Working on batch #:  162\n",
      "loss_on_batch: 0.01584712229669094  time_on_batch: 25.093711376190186\n",
      "Working on batch #:  163\n",
      "loss_on_batch: 0.019255228340625763  time_on_batch: 12.552371740341187\n",
      "Working on batch #:  164\n",
      "loss_on_batch: 0.030755311250686646  time_on_batch: 12.565404653549194\n",
      "Working on batch #:  165\n",
      "loss_on_batch: 0.022939734160900116  time_on_batch: 17.13655686378479\n",
      "Working on batch #:  166\n",
      "loss_on_batch: 0.01450288761407137  time_on_batch: 20.216744422912598\n",
      "Working on batch #:  167\n",
      "loss_on_batch: 0.016805989667773247  time_on_batch: 16.795657873153687\n",
      "Working on batch #:  168\n",
      "loss_on_batch: 0.014187564142048359  time_on_batch: 13.350488662719727\n",
      "Working on batch #:  169\n",
      "loss_on_batch: 0.019240472465753555  time_on_batch: 15.900268077850342\n",
      "Working on batch #:  170\n",
      "loss_on_batch: 0.020914098247885704  time_on_batch: 11.499572277069092\n",
      "Working on batch #:  171\n",
      "loss_on_batch: 0.03653458133339882  time_on_batch: 26.908531665802002\n",
      "Working on batch #:  172\n",
      "loss_on_batch: 0.02057480998337269  time_on_batch: 13.240195989608765\n",
      "Working on batch #:  173\n",
      "loss_on_batch: 0.037630338221788406  time_on_batch: 19.440682649612427\n",
      "Working on batch #:  174\n",
      "loss_on_batch: 0.030819157138466835  time_on_batch: 15.730817794799805\n",
      "Working on batch #:  175\n",
      "loss_on_batch: 0.015032978728413582  time_on_batch: 13.745541334152222\n",
      "Working on batch #:  176\n",
      "loss_on_batch: 0.019675910472869873  time_on_batch: 21.364795684814453\n",
      "Working on batch #:  177\n",
      "loss_on_batch: 0.016964992508292198  time_on_batch: 12.087133646011353\n",
      "Working on batch #:  178\n",
      "loss_on_batch: 0.02797248587012291  time_on_batch: 18.45004916191101\n",
      "Working on batch #:  179\n",
      "loss_on_batch: 0.020250000059604645  time_on_batch: 13.72749400138855\n",
      "Working on batch #:  180\n",
      "loss_on_batch: 0.02197662927210331  time_on_batch: 19.308329582214355\n",
      "Working on batch #:  181\n",
      "loss_on_batch: 0.023327400907874107  time_on_batch: 19.163946866989136\n",
      "Working on batch #:  182\n",
      "loss_on_batch: 0.024945493787527084  time_on_batch: 18.256534576416016\n",
      "Working on batch #:  183\n",
      "loss_on_batch: 0.023647556081414223  time_on_batch: 11.823430061340332\n",
      "Working on batch #:  184\n",
      "loss_on_batch: 0.03193323686718941  time_on_batch: 14.183706283569336\n",
      "Working on batch #:  185\n",
      "loss_on_batch: 0.04568427428603172  time_on_batch: 381.0731270313263\n",
      "Working on batch #:  186\n",
      "loss_on_batch: 0.024895550683140755  time_on_batch: 19.884084463119507\n",
      "Working on batch #:  187\n",
      "loss_on_batch: 0.01741907373070717  time_on_batch: 16.58737325668335\n",
      "Working on batch #:  188\n",
      "loss_on_batch: 0.027245081961154938  time_on_batch: 11.77684497833252\n",
      "Working on batch #:  189\n",
      "loss_on_batch: 0.009357224218547344  time_on_batch: 17.205175161361694\n",
      "Working on batch #:  190\n",
      "loss_on_batch: 0.02996319904923439  time_on_batch: 14.101479291915894\n",
      "Working on batch #:  191\n",
      "loss_on_batch: 0.024860309436917305  time_on_batch: 11.96982192993164\n",
      "Working on batch #:  192\n",
      "loss_on_batch: 0.008160973899066448  time_on_batch: 17.521578788757324\n",
      "Working on batch #:  193\n",
      "loss_on_batch: 0.027608221396803856  time_on_batch: 13.97515058517456\n",
      "Working on batch #:  194\n",
      "loss_on_batch: 0.02956211566925049  time_on_batch: 11.822428226470947\n",
      "Working on batch #:  195\n",
      "loss_on_batch: 0.018230419605970383  time_on_batch: 12.219483613967896\n",
      "Working on batch #:  196\n",
      "loss_on_batch: 0.016265869140625  time_on_batch: 11.585799217224121\n",
      "Working on batch #:  197\n",
      "loss_on_batch: 0.03291941061615944  time_on_batch: 125.00230550765991\n",
      "Working on batch #:  198\n",
      "loss_on_batch: 0.03431876748800278  time_on_batch: 15.197399854660034\n",
      "Working on batch #:  199\n",
      "loss_on_batch: 0.017643289640545845  time_on_batch: 17.496515035629272\n",
      "Working on batch #:  200\n",
      "loss_on_batch: 0.037140749394893646  time_on_batch: 23.22880220413208\n",
      "MRR score on dev set: 0.6507200493078177\n",
      "MRR score on test set: 0.6638035025874583\n",
      "MAP score on dev set: 0.518623149709\n",
      "MAP score on test set: 0.551597774855\n",
      "Precision at 1 score on dev set: 0.475\n",
      "Precision at 1 score on test set: 0.465\n",
      "Precision at 5 score on dev set: 0.3950000000000003\n",
      "Precision at 5 score on test set: 0.376\n",
      "Working on batch #:  201\n",
      "loss_on_batch: 0.04438186436891556  time_on_batch: 17.73887276649475\n",
      "Working on batch #:  202\n",
      "loss_on_batch: 0.015478059649467468  time_on_batch: 13.346482038497925\n",
      "Working on batch #:  203\n",
      "loss_on_batch: 0.013703199103474617  time_on_batch: 12.107185363769531\n",
      "Working on batch #:  204\n",
      "loss_on_batch: 0.03111330233514309  time_on_batch: 15.354817628860474\n",
      "Working on batch #:  205\n",
      "loss_on_batch: 0.027524026110768318  time_on_batch: 13.835846185684204\n",
      "Working on batch #:  206\n",
      "loss_on_batch: 0.025415360927581787  time_on_batch: 13.614498376846313\n",
      "Working on batch #:  207\n",
      "loss_on_batch: 0.021778207272291183  time_on_batch: 15.337319135665894\n",
      "Working on batch #:  208\n",
      "loss_on_batch: 0.017932655289769173  time_on_batch: 14.139116525650024\n",
      "Working on batch #:  209\n",
      "loss_on_batch: 0.01998659409582615  time_on_batch: 11.069954633712769\n",
      "Working on batch #:  210\n",
      "loss_on_batch: 0.04853663593530655  time_on_batch: 16.715877056121826\n",
      "Working on batch #:  211\n",
      "loss_on_batch: 0.023975137621164322  time_on_batch: 28.85392427444458\n",
      "Working on batch #:  212\n",
      "loss_on_batch: 0.019431518390774727  time_on_batch: 15.585432291030884\n",
      "Working on batch #:  213\n",
      "loss_on_batch: 0.021159419789910316  time_on_batch: 18.7335524559021\n",
      "Working on batch #:  214\n",
      "loss_on_batch: 0.008142040111124516  time_on_batch: 14.050317287445068\n",
      "Working on batch #:  215\n",
      "loss_on_batch: 0.031220663338899612  time_on_batch: 13.29025936126709\n",
      "Working on batch #:  216\n",
      "loss_on_batch: 0.01043955609202385  time_on_batch: 10.90480899810791\n",
      "Working on batch #:  217\n",
      "loss_on_batch: 0.017774729058146477  time_on_batch: 12.80678939819336\n",
      "Working on batch #:  218\n",
      "loss_on_batch: 0.019538721069693565  time_on_batch: 17.608763694763184\n",
      "Working on batch #:  219\n",
      "loss_on_batch: 0.019974935799837112  time_on_batch: 13.755030393600464\n",
      "Working on batch #:  220\n",
      "loss_on_batch: 0.022734392434358597  time_on_batch: 15.272107601165771\n",
      "Working on batch #:  221\n",
      "loss_on_batch: 0.03294319659471512  time_on_batch: 21.538257598876953\n",
      "Working on batch #:  222\n",
      "loss_on_batch: 0.01472968328744173  time_on_batch: 14.268055438995361\n",
      "Working on batch #:  223\n",
      "loss_on_batch: 0.029901107773184776  time_on_batch: 14.385008573532104\n",
      "Working on batch #:  224\n",
      "loss_on_batch: 0.01754472218453884  time_on_batch: 13.368603229522705\n",
      "Working on batch #:  225\n",
      "loss_on_batch: 0.024313658475875854  time_on_batch: 15.024940967559814\n",
      "Working on batch #:  226\n",
      "loss_on_batch: 0.02410329505801201  time_on_batch: 23.675941228866577\n",
      "Working on batch #:  227\n",
      "loss_on_batch: 0.03423899784684181  time_on_batch: 20.172728061676025\n",
      "Working on batch #:  228\n",
      "loss_on_batch: 0.025955284014344215  time_on_batch: 19.294291973114014\n",
      "Working on batch #:  229\n",
      "loss_on_batch: 0.019783606752753258  time_on_batch: 16.00003170967102\n",
      "Working on batch #:  230\n",
      "loss_on_batch: 0.02942337468266487  time_on_batch: 19.419625520706177\n",
      "Working on batch #:  231\n",
      "loss_on_batch: 0.028575628995895386  time_on_batch: 27.81321382522583\n",
      "Working on batch #:  232\n",
      "loss_on_batch: 0.030707059428095818  time_on_batch: 17.53685164451599\n",
      "Working on batch #:  233\n",
      "loss_on_batch: 0.023874379694461823  time_on_batch: 14.366176128387451\n",
      "Working on batch #:  234\n",
      "loss_on_batch: 0.009389277547597885  time_on_batch: 12.529284238815308\n",
      "Working on batch #:  235\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-33409e3879a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mquestions_this_training_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainingQuestionIds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Working on batch #: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestions_this_training_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation_val_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation_val_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-33409e3879a3>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(cnn, optimizer, batch_ids, batch_data, word2vec, id2Data, truncation_val_title, truncation_val_body)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mloss_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mloss_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 98\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' Model Specs '''\n",
    "# CNN parameters\n",
    "input_size = len(word2vec[list(word2vec.keys())[0]])\n",
    "hidden_size = 667\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation = 1\n",
    "groups = 1\n",
    "bias = True\n",
    "\n",
    "# CNN model\n",
    "#cnn = torch.nn.Sequential()\n",
    "#cnn.add_module('conv', torch.nn.Conv1d(in_channels = input_size, out_channels = hidden_size, kernel_size = kernel_size, padding = padding, dilation = dilation, groups = groups, bias = bias))\n",
    "#cnn.add_module('tanh', torch.nn.Tanh())\n",
    "#cnn.add_module('norm', torch.nn.BatchNorm1d(num_features = hidden_size))\n",
    "\n",
    "# Loss function\n",
    "#loss_function = torch.nn.MultiMarginLoss(margin=margin)\n",
    "\n",
    "# Optimizer\n",
    "#optimizer = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "''' Procedural parameters '''\n",
    "batch_size = 20\n",
    "num_differing_questions = 20\n",
    "num_epochs = 10\n",
    "num_batches = round(len(trainingQuestionIds)/batch_size)\n",
    "\n",
    "\n",
    "def train_model(cnn, optimizer, batch_ids, batch_data, word2vec, id2Data, truncation_val_title, truncation_val_body):\n",
    "    cnn.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    sequence_ids, dict_sequence_lengths = organize_ids_training(batch_ids, batch_data, num_differing_questions)\n",
    "\n",
    "    candidates_qs_tuples_matrix = construct_qs_matrix_training(sequence_ids, cnn, word2vec, id2Data, dict_sequence_lengths, input_size, num_differing_questions, truncation_val_title,\\\n",
    "                                                               truncation_val_body, candidates=True)\n",
    "    main_qs_tuples_matrix = construct_qs_matrix_training(batch_ids, cnn, word2vec, id2Data, dict_sequence_lengths, input_size, num_differing_questions, truncation_val_title,\\\n",
    "                                                         truncation_val_body, candidates=False)\n",
    "    similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-08)\n",
    "\n",
    "    target = Variable(torch.LongTensor([0] * int(len(sequence_ids)/(1+num_differing_questions))))\n",
    "    loss_batch = loss_function(similarity_matrix, target)\n",
    "\n",
    "    loss_batch.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"loss_on_batch:\", loss_batch.data[0], \" time_on_batch:\", time.time() - start)\n",
    "    return\n",
    "\n",
    "\n",
    "def eval_model(cnn, ids, data, word2vec, id2Data, truncation_val_title, truncation_val_body):\n",
    "    cnn.eval()\n",
    "    sequence_ids, p_pluses_indices_dict = organize_test_ids(ids, data)\n",
    "\n",
    "    candidates_qs_tuples_matrix = construct_qs_matrix_testing(sequence_ids, cnn, word2vec, id2Data, input_size, num_differing_questions, truncation_val_title, truncation_val_body,\\\n",
    "                                                              candidates=True)\n",
    "    main_qs_tuples_matrix = construct_qs_matrix_testing(ids, cnn, word2vec, id2Data, input_size, num_differing_questions, truncation_val_title, truncation_val_body, candidates=False)\n",
    "\n",
    "    similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-08)\n",
    "    MRR_score = get_MRR_score(similarity_matrix, p_pluses_indices_dict)\n",
    "    MAP_score = get_MAP_score(similarity_matrix, p_pluses_indices_dict)\n",
    "    avg_prec_at_1 = avg_precision_at_k(similarity_matrix, p_pluses_indices_dict, 1)\n",
    "    avg_prec_at_5 = avg_precision_at_k(similarity_matrix, p_pluses_indices_dict, 5) \n",
    "    return MRR_score, MAP_score, avg_prec_at_1, avg_prec_at_5\n",
    "\n",
    "\n",
    "'''Begin training'''\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Train on whole training data set\n",
    "    for batch in range(1, num_batches+1):\n",
    "        #if batch == 121 or batch == 98: # memory error with this batch\n",
    "        #    continue\n",
    "        start = time.time()\n",
    "        questions_this_training_batch = trainingQuestionIds[batch_size * (batch - 1):batch_size * batch]\n",
    "        print(\"Working on batch #: \", batch)\n",
    "        train_model(cnn, optimizer, questions_this_training_batch, training_data, word2vec, id2Data, truncation_val_title, truncation_val_body)\n",
    "        \n",
    "        if batch == 1 or batch % 50 == 0:\n",
    "            \n",
    "            # Evaluate on dev and test sets for MRR score\n",
    "\n",
    "            dev_scores = eval_model(cnn, dev_question_ids, dev_data, word2vec, id2Data, truncation_val_title, truncation_val_body)\n",
    "            test_scores = eval_model(cnn, test_question_ids, test_data, word2vec, id2Data, truncation_val_title, truncation_val_body)\n",
    "            print(\"MRR score on dev set:\", dev_scores[0])\n",
    "            print(\"MRR score on test set:\", test_scores[0])\n",
    "            print(\"MAP score on dev set:\", dev_scores[1])\n",
    "            print(\"MAP score on test set:\", test_scores[1])\n",
    "            print(\"Precision at 1 score on dev set:\", dev_scores[2])\n",
    "            print(\"Precision at 1 score on test set:\", test_scores[2])\n",
    "            print(\"Precision at 5 score on dev set:\", dev_scores[3])\n",
    "            print(\"Precision at 5 score on test set:\", test_scores[3])\n",
    "\n",
    "            # Log results to local logs.txt file\n",
    "            with open('logs_cnn2.txt', 'a') as log_file:\n",
    "                log_file.write('epoch: ' + str(epoch) + '\\n')\n",
    "                log_file.write('batch: ' + str(batch) + '\\n')\n",
    "                log_file.write('lr: ' + str(lr) +  ' marg: ' + str(margin) + '\\n' )        \n",
    "                log_file.write('dev_MRR: ' +  str(dev_scores[0]) + '\\n')\n",
    "                log_file.write('test_MRR: ' +  str(test_scores[0]) + '\\n')\n",
    "                log_file.write('dev_MAP: ' +  str(dev_scores[1]) + '\\n')\n",
    "                log_file.write('test_MAP: ' +  str(test_scores[1]) + '\\n')\n",
    "                log_file.write('dev_p_at_1: ' +  str(dev_scores[2]) + '\\n')\n",
    "                log_file.write('test_p_at_1: ' +  str(test_scores[2]) + '\\n')\n",
    "                log_file.write('dev_p_at_5: ' +  str(dev_scores[3]) + '\\n')\n",
    "                log_file.write('test_p_at_5: ' +  str(test_scores[3]) + '\\n')\n",
    "\n",
    "            # Save model for this epoch\n",
    "            torch.save(cnn, '../Pickle/' + saved_model_name + '_epoch' + str(epoch) + '_batch' + str(batch)+ '.pt')\n",
    "            # Save optimizer for this epoch\n",
    "            torch.save(optimizer, '../Pickle/' + 'optim_cnn2' + '_epoch' + str(epoch) + '_batch' + str(batch) +'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
